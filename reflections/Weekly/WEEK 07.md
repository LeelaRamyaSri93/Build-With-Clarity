# 🌿 Build With Clarity  
*A space to trace growth layer by layer, week by week.*

---
## Week 7 Reflection (Day 27 to Day 31)
---

This week marked a shift from raw data access to **structured extraction**. I moved beyond requests and into the realm of parsing learning how to interpret HTML, navigate tags, and build resilient scraping flows. From BeautifulSoup fundamentals to Scrapy’s architecture, I began shaping modular pipelines that can scale across websites and use cases.

---

### 🧠 BeautifulSoup: Parsing with Precision

I started by learning how to read HTML like a map identifying tags, classes, and IDs to extract meaningful data.

**What I learned:**
- How to use `.find()`, `.find_all()`, and `.attrs` for targeted extraction.
- How to clean data using `.text`, `.strip()`, and regex.
- How to handle edge cases and missing elements gracefully.
- That parsing is not just technical it’s interpretive.

---

### 🕸️ Scrapy: From Structure to Strategy

Once I understood HTML parsing, I stepped into Scrapy a framework built for scalable, structured scraping.

**What I learned:**
- How to scaffold spiders and use `response.css()` and `response.xpath()` for extraction.
- How to navigate Scrapy’s GitHub documentation and source code.
- How to handle pagination with `response.follow()` and recursive logic.
- That Scrapy is more than a tool—it’s a mindset for modular, maintainable scraping.

---

### 🎯 CSS Selectors: Targeting with Intent

I refined my selector logic to extract movie names, URLs, and nested elements with precision.

**What I learned:**
- How to combine selectors for clean, structured output.
- How to test and adapt selectors across different site layouts.
- That good selectors are the backbone of reliable spiders.

---

## 🧩 What Changed  
- I started thinking in layers: raw HTML → parsed structure → spider logic → exported data.  
- My scraping mindset evolved—from curiosity to craft, from scripts to systems.
