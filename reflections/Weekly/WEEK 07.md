# ğŸŒ¿ Build With Clarity  
*A space to trace growth layer by layer, week by week.*

---
## Week 7 Reflection (Day 27 to Day 31)
---

This week marked a shift from raw data access to **structured extraction**. I moved beyond requests and into the realm of parsing learning how to interpret HTML, navigate tags, and build resilient scraping flows. From BeautifulSoup fundamentals to Scrapyâ€™s architecture, I began shaping modular pipelines that can scale across websites and use cases.

---

### ğŸ§  BeautifulSoup: Parsing with Precision

I started by learning how to read HTML like a map identifying tags, classes, and IDs to extract meaningful data.

**What I learned:**
- How to use `.find()`, `.find_all()`, and `.attrs` for targeted extraction.
- How to clean data using `.text`, `.strip()`, and regex.
- How to handle edge cases and missing elements gracefully.
- That parsing is not just technical itâ€™s interpretive.

---

### ğŸ•¸ï¸ Scrapy: From Structure to Strategy

Once I understood HTML parsing, I stepped into Scrapy a framework built for scalable, structured scraping.

**What I learned:**
- How to scaffold spiders and use `response.css()` and `response.xpath()` for extraction.
- How to navigate Scrapyâ€™s GitHub documentation and source code.
- How to handle pagination with `response.follow()` and recursive logic.
- That Scrapy is more than a toolâ€”itâ€™s a mindset for modular, maintainable scraping.

---

### ğŸ¯ CSS Selectors: Targeting with Intent

I refined my selector logic to extract movie names, URLs, and nested elements with precision.

**What I learned:**
- How to combine selectors for clean, structured output.
- How to test and adapt selectors across different site layouts.
- That good selectors are the backbone of reliable spiders.

---

## ğŸ§© What Changed  
- I started thinking in layers: raw HTML â†’ parsed structure â†’ spider logic â†’ exported data.  
- My scraping mindset evolvedâ€”from curiosity to craft, from scripts to systems.
