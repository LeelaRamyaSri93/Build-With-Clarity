# 🌿 Build With Clarity  
*A space to trace growth layer by layer, week by week.*

---
## Week 4 Reflection (Day 14 to Day 18)

This week focused on applying foundational skills through a hands-on ETL pipeline project using Linux shell scripting, Python, and PostgreSQL. The goal was to simulate a real-world data flow from raw extraction to structured loading.

## 🧪 What I Built  
- A beginner-friendly ETL pipeline using:
  - Shell scripting for orchestration
  - Python for data transformation
  - PostgreSQL for data storage and querying
- Dataset: Drug labels and side effects (Kaggle)

## 🧭 What I Learned  
- ETL isn’t just a concept, building it phase by phase gave me clarity on how data moves and transforms.
- Linux commands became more meaningful when used in a real workflow.
- PostgreSQL integration helped reinforce SQL fluency from Week 3.
- Structuring the repo with purpose made the project easier to share and scale.

## 📁 Repo Structure  
- `scripts/`: Shell and Python scripts for each ETL phase  
- `data/`: Raw and processed CSV files  
- `sql/`: Database schema and queries  
- `docs/`: Phase-wise documentation and visuals  

## 📚 Documentation  
Each phase is documented in detail:

- [Extract Phase](https://github.com/LeelaRamyaSri93/Linux_ETL_Pipeline/blob/main/docs/Extract_Phase.md)  
- [Transform Phase](https://github.com/LeelaRamyaSri93/Linux_ETL_Pipeline/blob/main/docs/Transform_Phase.md)  
- [Load Phase](https://github.com/LeelaRamyaSri93/Linux_ETL_Pipeline/blob/main/docs/Load_Phase.md)  

## 🖼️ Visual Reference  
![ETL Workflow](https://github.com/LeelaRamyaSri93/Linux_ETL_Pipeline/blob/main/docs/Linux_ETL_Pipeline_WORKFLOW.png)

## 🔗 Repository  
[Linux_ETL_Pipeline](https://github.com/LeelaRamyaSri93/Linux_ETL_Pipeline/tree/main)

> "This wasn’t a complex project but it was complete. Building it myself gave me the confidence to design more pipelines from different data sources, with deeper clarity and control."
