# 🌿 Build With Clarity  
*A space to trace growth layer by layer, week by week.*

---
## Week 6 Reflection (Day 24 to Day 26)

---

I just completed my **foundational phase** a deep dive into systems, security, and cloud architecture. Now I’ve stepped into the **core workflows phase**, where the focus shifts from setup to flow: how data moves, how tools connect, and how logic becomes pipelines. 
This week was all about preparing the ground for data engineering understanding how to access data, and how to build modular scraping flows.

---

### 🕸️ Data Scraping: From Curiosity to Craft

I began with the “why” behind scraping understanding the ethics, the structure of websites, and the tools that make it possible.

**What I learned:**
- How to identify static vs dynamic sites, and when to use `requests`, `Scrapy`, or `Selenium`.
- How to design scraping flows that mimic human behavior (headers, delays, retries).
- That scraping is a form of negotiation: between what’s available and what’s allowed.

---

### 🧪 Python Requests: From Calls to Control

Once I understood the landscape, I dove into the `requests` module. It became gateway to APIs and HTML pages lightweight, flexible, and powerful.

**What I learned:**
- How to send GET and POST requests with custom headers and query parameters.
- How to handle pagination, timeouts, and session persistence.
- How to modularize request logic for reuse across scripts.
- That every request is a handshake status codes tell the story.

---

## 🧩 What Changed  
- I started thinking in flows: `requests` → raw HTML → (next: parsing) → cleaned data → reusable modules.  

> Next week, I’ll begin parsing workflows and dive into HTML structure analysis.
